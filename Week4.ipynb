{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "##Importing Libraries and mounting google drive."
      ],
      "metadata": {
        "id": "VqjNQ5yJUldz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Mounting google drive to access documents\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z1ODcirpOIUZ",
        "outputId": "fec3de5a-bfc5-4bf9-97a9-227dcb9807d1"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import re\n",
        "import numpy as np\n",
        "from collections import defaultdict\n",
        "from math import log"
      ],
      "metadata": {
        "id": "EoafOa7X4i7Z"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Preprocessing text"
      ],
      "metadata": {
        "id": "ZuZnMMEjZNeU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Preprocessing function to clean and tokenize text\n",
        "def preprocess(text):\n",
        "    return re.findall(r'\\b\\w+\\b', text.lower())"
      ],
      "metadata": {
        "id": "6H-6x5OsMwke"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Documents and Queries are loaded\n"
      ],
      "metadata": {
        "id": "GpAT9fjDZWAh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Load documents from the provided folder path\n",
        "def load_documents(folder_path):\n",
        "    docs = {}\n",
        "    for filename in os.listdir(folder_path):\n",
        "        if filename.endswith('.txt'):\n",
        "            with open(os.path.join(folder_path, filename), 'r') as file:\n",
        "                docs[filename] = preprocess(file.read())\n",
        "    return docs"
      ],
      "metadata": {
        "id": "KRtxNgD6M1_g"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Loading queries from the provided query file\n",
        "def load_queries(query_file_path):\n",
        "    with open(query_file_path, 'r') as file:\n",
        "        return [line.strip() for line in file.readlines()]"
      ],
      "metadata": {
        "id": "7L4rcDYqM5_x"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Computing Term Frequencies and Document Frequencies"
      ],
      "metadata": {
        "id": "hwQsfK11ZaVD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Computing term frequencies and document frequencies for each word in the documents\n",
        "def compute_statistics(docs):\n",
        "    doc_count = len(docs)\n",
        "    term_doc_freq = defaultdict(int) #Counting how many docs contain each term\n",
        "    term_freq = defaultdict(lambda: defaultdict(int)) #Counting term frequency in each document\n",
        "\n",
        "    for doc_id, words in docs.items():\n",
        "        word_set = set(words) #Getting unique words in the document\n",
        "        for word in words:\n",
        "            term_freq[doc_id][word] += 1 #Counting occurrences of each word\n",
        "        for word in word_set:\n",
        "            term_doc_freq[word] += 1 #Counting how many documents contain the word\n",
        "\n",
        "    return term_freq, term_doc_freq, doc_count"
      ],
      "metadata": {
        "id": "ht5wobpaM-Ql"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Computing Relevance Probabilities using BIM\n"
      ],
      "metadata": {
        "id": "GXyzbkz8Zssu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Compute relevance probabilities using BIM\n",
        "def compute_relevance_prob(query, term_freq, term_doc_freq, doc_count):\n",
        "    scores = {}\n",
        "    for doc_id in term_freq:\n",
        "        score = 1.0 #Initializing score for each document\n",
        "        for term in query:\n",
        "            tf = term_freq[doc_id].get(term, 0) #Getting term frequency in the document\n",
        "            df = term_doc_freq.get(term, 0) #Getting document frequency of the term\n",
        "            #Calculating probability of the term being relevant\n",
        "            p_term_given_relevant = (tf + 1) / (sum(term_freq[doc_id].values()) + len(term_doc_freq))\n",
        "            #Calculating probability of the term being non-relevant\n",
        "            p_term_given_not_relevant = (df + 1) / (doc_count - df + len(term_doc_freq))\n",
        "            score *= (p_term_given_relevant / p_term_given_not_relevant) #Updating document score\n",
        "        scores[doc_id] = score\n",
        "    return scores"
      ],
      "metadata": {
        "id": "Xbo4ul0-NAmz"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Retrieving and Ranking Documents"
      ],
      "metadata": {
        "id": "f4cTL6DIZwXA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Retrieving documents based on queries and ranking them by relevance scores\n",
        "def retrieve_documents(folder_path, query_file_path):\n",
        "    docs = load_documents(folder_path)\n",
        "    queries = load_queries(query_file_path)\n",
        "\n",
        "    term_freq, term_doc_freq, doc_count = compute_statistics(docs)\n",
        "\n",
        "    for query in queries:\n",
        "        query_terms = preprocess(query)  # Tokenizing the query\n",
        "        scores = compute_relevance_prob(query_terms, term_freq, term_doc_freq, doc_count)\n",
        "        ranked_docs = sorted(scores.items(), key=lambda item: item[1], reverse=True)  # Ranking docs by score\n",
        "\n",
        "        # Printing top 3 ranked documents in a specified format\n",
        "        print(f\"Top 3 Relevance Scores for query {query}:\")\n",
        "        for rank, (doc_id, score) in enumerate(ranked_docs[:3], 1):\n",
        "            print(f\"Rank {rank}: {doc_id}, Score: {score:.4f}\")\n",
        "        print()"
      ],
      "metadata": {
        "id": "Wje2jwEFNCNd"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Example usage to retrieve documents based on queries\n",
        "folder_path = '/content/drive/MyDrive/Project Dataset'\n",
        "query_file_path = '/content/drive/MyDrive/quert.txt/query.txt'\n",
        "retrieve_documents(folder_path, query_file_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-5eF9_erNDnD",
        "outputId": "fc7683fb-cee6-4c4e-f686-e5797382530c"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top 3 Relevance Scores for query Patient:\n",
            "Rank 1: Pacemaker Implantation.txt, Score: 0.0850\n",
            "Rank 2: Spinal Infusion.txt, Score: 0.0848\n",
            "Rank 3: Breast Augmentation.txt, Score: 0.0844\n",
            "\n",
            "Top 3 Relevance Scores for query Surgery:\n",
            "Rank 1: Cataract Surgery.txt, Score: 0.1999\n",
            "Rank 2: Spinal Infusion.txt, Score: 0.1696\n",
            "Rank 3: Spinal Fusion Surgery.txt, Score: 0.1646\n",
            "\n",
            "Top 3 Relevance Scores for query Complication:\n",
            "Rank 1: Pacemaker Implantation.txt, Score: 0.1274\n",
            "Rank 2: Cesarean Section.txt, Score: 0.1243\n",
            "Rank 3: Spinal Infusion.txt, Score: 0.0848\n",
            "\n",
            "Top 3 Relevance Scores for query Treatment:\n",
            "Rank 1: Liver Transplant.txt, Score: 0.1243\n",
            "Rank 2: Mastectomy.txt, Score: 0.1237\n",
            "Rank 3: Hernia Repair.txt, Score: 0.1211\n",
            "\n",
            "Top 3 Relevance Scores for query Outcome:\n",
            "Rank 1: Hernia Repair.txt, Score: 0.1211\n",
            "Rank 2: Pacemaker Implantation.txt, Score: 0.0850\n",
            "Rank 3: Spinal Infusion.txt, Score: 0.0848\n",
            "\n",
            "Top 3 Relevance Scores for query Recovery:\n",
            "Rank 1: Spinal Infusion.txt, Score: 0.3422\n",
            "Rank 2: ACL Reconstruction.txt, Score: 0.3354\n",
            "Rank 3: Spinal Fusion Surgery.txt, Score: 0.3321\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Assigning and Saving Random Relevance Scores"
      ],
      "metadata": {
        "id": "zkMf1zgbZ2X1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "#Assigning random relevance scores (0 for irrelevant, 1 for relevant)\n",
        "def assign_random_relevance(queries, documents, relevance_scale=(0, 1)):\n",
        "    relevance_scores = {}\n",
        "\n",
        "    for query in queries:\n",
        "        relevance_scores[query] = {}  #Initializing relevance score dictionary for the query\n",
        "        for doc in documents:\n",
        "            #Assigning a random relevance score (between 0 and 1 by default)\n",
        "            relevance_scores[query][doc] = random.randint(relevance_scale[0], relevance_scale[1])\n",
        "\n",
        "    return relevance_scores\n",
        "\n",
        "#Saving the relevance scores to a file\n",
        "def save_relevance_scores_to_file(relevance_scores, output_file):\n",
        "    with open(output_file, 'w') as f:\n",
        "        for query, doc_scores in relevance_scores.items():\n",
        "            for doc, score in doc_scores.items():\n",
        "                f.write(f\"{query},{doc},{score}\\n\")  #Saving as query,document,score format\n",
        "\n",
        "\n",
        "#Example usage to assign random relevance scores and saving them to a file\n",
        "folder_path = '/content/drive/MyDrive/Project Dataset'\n",
        "query_file_path = '/content/drive/MyDrive/quert.txt/query.txt'\n",
        "\n",
        "#Loading documents and queries\n",
        "documents = load_documents(folder_path)  #Returning a dictionary of document_id -> content\n",
        "queries = load_queries(query_file_path)  #Returning a list of queries\n",
        "\n",
        "#Randomly assigning relevance scores (0 = irrelevant, 1 = relevant)\n",
        "random_relevance_scores = assign_random_relevance(queries, documents.keys())\n",
        "\n",
        "#Saving the relevance scores to query_relevance_score.txt\n",
        "output_file = 'query_relevance_score.txt'\n",
        "save_relevance_scores_to_file(random_relevance_scores, output_file)\n",
        "\n",
        "print(f\"Relevance scores saved to {output_file}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dXuuB0rtW3Cn",
        "outputId": "8dacee99-7cdc-4b80-d095-3c886d88e3b0"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Relevance scores saved to query_relevance_score.txt\n"
          ]
        }
      ]
    }
  ]
}